{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# Citing Relevant Assets\n",
    "# This project utilizes several open-source libraries and assets:\n",
    "# 1. Pre-trained models are sourced from Huggingface and Stable Baselines3, \n",
    "#    specifically the Proximal Policy Optimization (PPO) models: \"sb3/ppo-CartPole-v1\" and \"sb3/ppo-LunarLander-v2\"\n",
    "#    and \"sb3/ppo-LunarLander-v2\" (https://huggingface.co/sb3).\n",
    "# 2. The PySR library is employed for symbolic regression: \n",
    "#    Cranmer, Miles, et al., \"PySR: Fast & Scalable Symbolic Regression in Python\" (2023), \n",
    "#    available at https://github.com/MilesCranmer/PySR.\n",
    "# 3. For Reinforcement Learning environments, the Gym library from OpenAI is used: \n",
    "#    Brockman, G., et al., \"OpenAI Gym\", available at https://github.com/openai/gym.\n",
    "# 4. The project is conducted in the context of the lecture Advanced Topics in Reinforcement Learning \n",
    "#    at Gottfried Wilhelm Leibniz UniversitÃ¤t Hannover, under Prof. Dr. Marius Lindauer and Theresa Eimer (2024).\n",
    "# \n",
    "# Ensure to properly cite these resources in any external use or publication derived from this work.\n",
    "# -----------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hv8G7tdRDViv"
   },
   "outputs": [],
   "source": [
    "!pip install cython==3.0.10\n",
    "!pip install swig==4.2.1\n",
    "!pip install cmake==3.27.9\n",
    "!pip install numpy==1.24.2\n",
    "!pip install tqdm==4.66.4\n",
    "!pip install torch==2.2.2\n",
    "!pip install stable-baselines3[extra]==2.3.2\n",
    "!pip install huggingface_sb3==3.0\n",
    "!pip install pysr==0.19.4\n",
    "!pip install carl-bench==1.1.0A\n",
    "!pip install pygame==2.6.0\n",
    "!pip install ffmpeg==1.4\n",
    "!pip install gymnasium[box2d]>=0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIyjAuQSDSoo"
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from pysr import PySRRegressor\n",
    "import sympy as sp\n",
    "from huggingface_sb3 import load_from_hub\n",
    "from google.colab import drive\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*progress bar will be turned off.*\")\n",
    "\n",
    "# Determine computation device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_pretrained_model(filename, repo_id, repo_filename):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained PPO model from Hugging Face or local storage.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Local filename to check or save the model.\n",
    "    repo_id (str): Hugging Face repo ID for downloading the model.\n",
    "    repo_filename (str): Name of the model file in the repo.\n",
    "\n",
    "    Returns:\n",
    "    model: Loaded PPO model.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(filename):\n",
    "        filename_ = load_from_hub(repo_id=repo_id, filename=repo_filename).replace(\".zip\", \"\")\n",
    "        print(f\"Model retrieved from Hugging Face: {filename_}\")\n",
    "        model = PPO.load(filename_)\n",
    "        model.save(filename)\n",
    "    else:\n",
    "        model = PPO.load(filename)\n",
    "\n",
    "    print(f\"Model exists locally: {os.path.isfile(filename)}\")\n",
    "    return model\n",
    "\n",
    "def customize_environment(env):\n",
    "    \"\"\"\n",
    "    Customize the environment parameters as needed.\n",
    "\n",
    "    Parameters:\n",
    "    env: Gym environment to be customized.\n",
    "\n",
    "    Returns:\n",
    "    Customized environment (if applicable).\n",
    "    \"\"\"\n",
    "    # Modify environment settings if needed (e.g., turbulence in other environments)\n",
    "    return env\n",
    "\n",
    "def sample_observations(env, n_samples):\n",
    "    \"\"\"\n",
    "    Samples random observations from the environment's observation space.\n",
    "\n",
    "    Parameters:\n",
    "    env: Gym environment.\n",
    "    n_samples (int): Number of samples to collect.\n",
    "\n",
    "    Returns:\n",
    "    List of sampled observations.\n",
    "    \"\"\"\n",
    "    return [env.observation_space.sample() for _ in range(n_samples)]\n",
    "\n",
    "def get_action_values(model, observations, num_actions):\n",
    "    \"\"\"\n",
    "    Computes action values for the provided observations using the model.\n",
    "\n",
    "    Parameters:\n",
    "    model: Trained model to compute action values.\n",
    "    observations: List of observations to evaluate.\n",
    "    num_actions (int): Number of available actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "    List of computed action values.\n",
    "    \"\"\"\n",
    "    action_samples = []\n",
    "    with tqdm(total=len(observations), desc=\"Sampling action values\") as pbar:\n",
    "        for obs in observations:\n",
    "            pbar.update(1)\n",
    "            action_logits = model.policy.forward(torch.tensor(obs, dtype=torch.float32).to(device).view(1, -1))[0]\n",
    "            action_samples.append(torch.nn.functional.one_hot(action_logits, num_actions).cpu().detach().numpy().flatten())\n",
    "\n",
    "    return action_samples\n",
    "\n",
    "def collect_inference_samples(env, model, n_samples_inference, max_steps, num_actions):\n",
    "    \"\"\"\n",
    "    Collects state-action pairs via inference on the environment using the model.\n",
    "\n",
    "    Parameters:\n",
    "    env: Gym environment.\n",
    "    model: Trained model for inference.\n",
    "    n_samples_inference (int): Number of inference samples to collect.\n",
    "    max_steps (int): Maximum steps allowed per episode.\n",
    "    num_actions (int): Number of possible actions.\n",
    "\n",
    "    Returns:\n",
    "    Numpy arrays of observations and corresponding action values.\n",
    "    \"\"\"\n",
    "    observations, action_values = [], []\n",
    "    samples_inference = 0\n",
    "    with tqdm(total=n_samples_inference, desc=\"Collecting inference samples\") as pbar:\n",
    "        while samples_inference < n_samples_inference:\n",
    "            obs = env.reset(seed=seed)\n",
    "            for _ in range(max_steps):\n",
    "                action_logits = model.policy.forward(torch.tensor(obs, dtype=torch.float32).to(device).view(1, -1))[0]\n",
    "                action_logits = torch.nn.functional.one_hot(action_logits, num_actions).cpu().detach().numpy().flatten()\n",
    "\n",
    "                if np.random.random() < sample_point_probability:\n",
    "                    observations.append(obs)\n",
    "                    action_values.append(action_logits)\n",
    "                    samples_inference += 1\n",
    "                    pbar.update(1)\n",
    "                    if samples_inference >= n_samples_inference:\n",
    "                        break\n",
    "\n",
    "                action = np.argmax(action_logits)\n",
    "                obs, _, done, truncated = env.step(action)\n",
    "                if done or truncated:\n",
    "                    break\n",
    "\n",
    "    return np.array(observations), np.array(action_values)\n",
    "\n",
    "def retrieve_symbolic_models(observations, action_values, num_actions, passive_alternative_action=False):\n",
    "    \"\"\"\n",
    "    Retrieves symbolic models for each action using symbolic regression.\n",
    "\n",
    "    Parameters:\n",
    "    observations: Numpy array of environment observations.\n",
    "    action_values: Numpy array of corresponding action values.\n",
    "    num_actions (int): Number of actions.\n",
    "    passive_alternative_action (bool): Whether to consider passive alternative actions.\n",
    "\n",
    "    Returns:\n",
    "    List of symbolic models.\n",
    "    \"\"\"\n",
    "    symbolic_models = []\n",
    "    if passive_alternative_action:\n",
    "        action_values = action_values[:, :-1]\n",
    "        observations = observations[:, :-1]\n",
    "\n",
    "    for i in range(action_values.shape[1]):\n",
    "        print(f\"Retrieving symbolic model for action {i + 1}\")\n",
    "        np.random.seed(seed)\n",
    "        model_simplified = PySRRegressor(\n",
    "            procs=0, multithreading=False, random_state=seed,\n",
    "            niterations=n_pySR_iterations,\n",
    "            binary_operators=[\"+\", \"-\", \"*\", \"/\", \"^\"],\n",
    "            unary_operators=[\"cos\", \"sin\", \"exp\", \"relu\", \"tanh\", \"atanh_clip\", \"sqrt\", \"cosh\", \"abs\", \"log\"],\n",
    "            constraints={'^': (-1, 1)},\n",
    "            extra_sympy_mappings={\"Heaviside\": lambda x: x > 0},\n",
    "            model_selection=\"accuracy\",\n",
    "            elementwise_loss=\"L2DistLoss()\",\n",
    "            maxsize=25,\n",
    "            populations=50,\n",
    "            parsimony=0.0\n",
    "        )\n",
    "        model_simplified.fit(observations, action_values[:, i])\n",
    "        symbolic_models.append(model_simplified)\n",
    "        print(f\"Best symbolic model for action {i}: {model_simplified.get_best()}\")\n",
    "\n",
    "    return symbolic_models\n",
    "\n",
    "def evaluate_symbolic_expression(symbolic_expression, observation):\n",
    "    \"\"\"\n",
    "    Evaluates a symbolic expression for a given observation.\n",
    "\n",
    "    Parameters:\n",
    "    symbolic_expression: Symbolic expression to evaluate.\n",
    "    observation: Environment observation as input.\n",
    "\n",
    "    Returns:\n",
    "    Evaluation result as a float.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = symbolic_expression.subs({f\"x{j}\": observation[j] for j in range(len(observation))}).evalf()\n",
    "        return float(result) if result.is_real else float(result.as_real_imag()[0])\n",
    "    except TypeError as e:\n",
    "        print(f\"Error evaluating symbolic expression: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "class SymbolicPolicyAgent:\n",
    "    def __init__(self, symbolic_models, num_actions, model_index=0, passive_alternative_action=False):\n",
    "        \"\"\"\n",
    "        Initializes the symbolic policy agent.\n",
    "\n",
    "        Parameters:\n",
    "        symbolic_models: List of symbolic models for actions.\n",
    "        num_actions (int): Number of actions.\n",
    "        model_index (int): Index for selecting which model to use.\n",
    "        passive_alternative_action (bool): If true, includes a passive alternative action in decision-making.\n",
    "        \"\"\"\n",
    "        self.symbolic_models = symbolic_models\n",
    "        self.model_index = model_index\n",
    "        self.symbolic = [self.symbolic_models[i].sympy(max(0, len(self.symbolic_models[i].equations_) - self.model_index - 1)) for i in range(len(self.symbolic_models))]\n",
    "        self.passive_alternative_action = passive_alternative_action\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        \"\"\"\n",
    "        Selects an action based on symbolic models and observation.\n",
    "\n",
    "        Parameters:\n",
    "        observation: Current observation from the environment.\n",
    "\n",
    "        Returns:\n",
    "        Selected action index.\n",
    "        \"\"\"\n",
    "        action_scores = [evaluate_symbolic_expression(expr, observation) for expr in self.symbolic]\n",
    "        if self.passive_alternative_action:\n",
    "            action_scores.append(0.5)\n",
    "        return np.argmax(action_scores)\n",
    "\n",
    "    def set_model_index(self, model_index):\n",
    "        \"\"\"\n",
    "        Sets a new model index for symbolic evaluation.\n",
    "\n",
    "        Parameters:\n",
    "        model_index (int): Index of the model to use.\n",
    "        \"\"\"\n",
    "        self.model_index = model_index\n",
    "        self.symbolic = [self.symbolic_models[i].sympy(len(self.symbolic_models[i].equations_) - 1 - self.model_index) for i in range(len(self.symbolic_models))]\n",
    "\n",
    "def evaluate_symbolic_agent(env, agent, n_episodes):\n",
    "    \"\"\"\n",
    "    Evaluates the symbolic policy agent over multiple episodes.\n",
    "\n",
    "    Parameters:\n",
    "    env: Gym environment.\n",
    "    agent: Symbolic policy agent to evaluate.\n",
    "    n_episodes (int): Number of episodes to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    Mean and standard deviation of rewards, and list of rewards.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for _ in trange(n_episodes, desc=\"Evaluating agent\"):\n",
    "        obs = env.reset()\n",
    "        done, episode_reward = False, 0\n",
    "        while not done:\n",
    "            action = agent.select_action(obs)\n",
    "            obs, reward, done, truncated = env.step(action)\n",
    "            done = done or truncated\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "\n",
    "    return np.mean(rewards), np.std(rewards), rewards\n",
    "\n",
    "def evaluate_and_plot_models(env, symbolic_models, num_actions, evaluated_episodes):\n",
    "    \"\"\"\n",
    "    Evaluates and plots performance of symbolic models.\n",
    "\n",
    "    Parameters:\n",
    "    env: Gym environment.\n",
    "    symbolic_models: List of symbolic models to evaluate.\n",
    "    num_actions (int): Number of actions in the environment.\n",
    "    evaluated_episodes (int): Number of episodes to evaluate per model.\n",
    "\n",
    "    Returns:\n",
    "    List of mean rewards, standard deviations, and rewards for each model.\n",
    "    \"\"\"\n",
    "    results, all_rewards = [], []\n",
    "    for k in range(len(symbolic_models[0].equations_)):\n",
    "        try:\n",
    "            symbolic_agent = SymbolicPolicyAgent(symbolic_models, num_actions, model_index=k)\n",
    "            mean_reward, std_reward, rewards = evaluate_symbolic_agent(env, symbolic_agent, evaluated_episodes)\n",
    "            results.append((mean_reward, std_reward))\n",
    "            all_rewards.append(rewards)\n",
    "\n",
    "            # Plotting results\n",
    "            k_indices = np.arange(len(results))\n",
    "            mean_rewards = [result[0] for result in results]\n",
    "            std_rewards = [result[1] for result in results]\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(k_indices, mean_rewards, label=\"Mean Reward\", color='royalblue')\n",
    "            plt.fill_between(k_indices, np.array(mean_rewards) - np.array(std_rewards), np.array(mean_rewards) + np.array(std_rewards), color='royalblue', alpha=0.3)\n",
    "            plt.xlabel(\"k-best model index\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.title(f\"Mean Reward and Confidence Interval for k-best Models (seed: {seed})\")\n",
    "            plt.legend()\n",
    "            plt.xticks(k_indices)\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"No entries for {k+1}th best model: {e}\")\n",
    "\n",
    "    return results, all_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXRNpOUQzZM0"
   },
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "\n",
    "# Connect to Google Drive if storing results in Drive is desired\n",
    "use_drive = True\n",
    "directory = \"ATRL/\"\n",
    "if use_drive:\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    storage_path = '/content/drive/MyDrive/' + directory\n",
    "else:\n",
    "    storage_path = directory\n",
    "\n",
    "# Ensure storage directory exists\n",
    "os.makedirs(storage_path) if not os.path.exists(storage_path) else print(storage_path + \" already exists\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 1000\n",
    "\n",
    "# Configuration Parameters\n",
    "sample_point_probability = 0.1  # Probability of sampling data points (higher = more diverse samples, but more time)\n",
    "evaluated_episodes = 25         # Number of episodes to evaluate the model's performance (higher = more precise results)\n",
    "n_pySR_iterations = 100         # Number of iterations for symbolic regression (increase for better results)\n",
    "n_samples_inference = 10_000    # Number of inference samples (upper limit for pySR framework)\n",
    "designation = f\"cartpole_s{seed}_both_actions_represented\"\n",
    "\n",
    "# Load pre-trained model from Hugging Face or local storage\n",
    "trained_policy_filename = \"ppo-CartPole-v1.zip\"\n",
    "repo_id = \"sb3/ppo-CartPole-v1\"\n",
    "repo_filename = \"ppo-CartPole-v1.zip\"\n",
    "model = load_pretrained_model(trained_policy_filename, repo_id, repo_filename)\n",
    "\n",
    "# Load and customize the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = customize_environment(env)\n",
    "num_actions = 2  # CartPole has two actions: left (0) and right (1)\n",
    "max_steps = 500  # Maximum steps per episode in CartPole\n",
    "\n",
    "# Set seeds for reproducibility across libraries\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.reset(seed=seed)\n",
    "\n",
    "# Collect inference samples from the environment\n",
    "observations, action_values = collect_inference_samples(env, model, n_samples_inference, max_steps, num_actions)\n",
    "\n",
    "# Retrieve symbolic models using the collected data\n",
    "symbolic_models = retrieve_symbolic_models(observations, action_values, num_actions)\n",
    "\n",
    "# Store symbolic models to disk\n",
    "symbolic_model_filename = storage_path + f'pysr_models_{designation}.pkl'\n",
    "with open(symbolic_model_filename, 'wb') as f:\n",
    "    pickle.dump(symbolic_models, f)\n",
    "\n",
    "# Evaluate the symbolic models and plot performance\n",
    "results, all_rewards = evaluate_and_plot_models(env, symbolic_models, num_actions, evaluated_episodes)\n",
    "\n",
    "# Store individual episode rewards to disk\n",
    "rewards_filename = storage_path + f'episode_rewards_{designation}.pkl'\n",
    "with open(rewards_filename, 'wb') as f:\n",
    "    pickle.dump(all_rewards, f)\n",
    "\n",
    "# Close the environment to free resources\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ji3UmWFoXmXC"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
